{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GET AND PREPARE DATA FOR LOCALIZED-POD-DL-ROM\n",
    "This code performes the clustering to find n-clusters POD basis to use the POD-DL-ROM algorithms.\n",
    "Notice that this algorithm is reduced to usual POD-DL-ROM setting n-clusters = 1. The POD-DL-ROM architecture is explained here https://www.sciencedirect.com/science/article/pii/S0045782521005120. One could also only use only the clustering part to study the classifier. Use the blocks in the order it is indicated. If there are different possible routes you will find direction a or b, follow just one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import scipy.interpolate\n",
    "import os\n",
    "import csv\n",
    "import copy\n",
    "import pandas as pd\n",
    "from scipy.interpolate import griddata\n",
    "from numpy import savetxt\n",
    "import sklearn.utils.extmath\n",
    "import progressbar\n",
    "from fcmeans import FCM\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "def get_field(path,simulations,seconds,mesh,field):\n",
    "    Ns = len(simulations)*len(seconds)\n",
    "    S = np.zeros((Ns,mesh))\n",
    "    row = 0\n",
    "    bar = progressbar.ProgressBar(maxval=len(simulations), \\\n",
    "    widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n",
    "    bar.start()\n",
    "    for i,s in enumerate(simulations):\n",
    "        bar.update(i+1)\n",
    "        for t in seconds:\n",
    "            data = get_data(path+'/'+'sim_'+str(s)+'/'+str(t)+'/'+field,mesh)\n",
    "            S[row,:] = data\n",
    "            row+=1\n",
    "        sleep(0.1)\n",
    "    bar.finish()\n",
    "    return S\n",
    "\n",
    "def get_data(path,mesh):\n",
    "    with open(path, 'r') as file:\n",
    "        output = [None]*mesh\n",
    "        count = 0\n",
    "        start = False\n",
    "        for line in file:\n",
    "            #print(str(line))\n",
    "            if line[0] == ')':\n",
    "                break\n",
    "            if start:\n",
    "                output[count] = (float(line))\n",
    "                #print(output[count])\n",
    "                count +=1\n",
    "            if line[0] == \"(\":\n",
    "                start = True\n",
    "        return np.array(output)\n",
    "\n",
    "def read_files_centers(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        if file_path[-1] != 'U':\n",
    "            lines = np.array(file.read().split(\"\\n\"))\n",
    "            start = np.where(lines == \"(\")\n",
    "            end = np.where(lines == \")\")\n",
    "            if len(start[0]) != 0:\n",
    "                output = np.array(lines[start[0][0]+1:end[0][0]])\n",
    "                return output\n",
    "            else:\n",
    "                return []\n",
    "        else:\n",
    "            U = read_U(file_path)\n",
    "            return U\n",
    "\n",
    "def M_matrix(simulations,time,parameters,path):\n",
    "    file = open(path+'/X_LHS_Uniform.csv')\n",
    "    csvreader = csv.reader(file)\n",
    "    #header = next(csvreader)\n",
    "    #print(header)\n",
    "    rows = []\n",
    "    for count,row in enumerate(csvreader):\n",
    "        if count+1<=simulations:\n",
    "            rows.append(row)\n",
    "        else: \n",
    "            break\n",
    "    file.close()\n",
    "    M = np.zeros((simulations*len(time),parameters))#parameters = parameters+1\n",
    "    count = 0\n",
    "    for i in rows:\n",
    "        for t in time:\n",
    "            first = np.array([t])\n",
    "            second = np.array(i)\n",
    "            #print(count*time+t)\n",
    "            M[count,:] = np.concatenate([first,second])\n",
    "            count+=1\n",
    "    return M\n",
    "\n",
    "def normalize_M(M,path):\n",
    "    M_norm = np.zeros((np.shape(M)[0],np.shape(M)[1]))\n",
    "    M_max = np.amax(M, axis=0)\n",
    "    M_min = -np.amax(-M,axis=0)\n",
    "    maxmin = {'max_par':M_max,'min_par':M_min}\n",
    "    df = pd.DataFrame(maxmin,dtype = 'float')\n",
    "    df.to_csv(path+'normalization.csv')\n",
    "    for i in range(np.shape(M)[0]):\n",
    "        for j in range(np.shape(M)[1]):\n",
    "            M_norm[i,j] = (M[i,j]- M_min[j])/(M_max[j]-M_min[j])\n",
    "    return M_norm\n",
    "\n",
    "def normalize_S(S):\n",
    "    S_max = np.amax(S)\n",
    "    S_min = -np.amax(-S)\n",
    "    print(S_max)\n",
    "    print(S_min)\n",
    "    return (S-S_min)/(S_max-S_min)\n",
    "\n",
    "def normalize_Input(Input,path,maxima,minima,n_clusters,labels):\n",
    "    Input_norm = np.zeros((np.shape(Input)[0], np.shape(Input)[1]))\n",
    "    for count,i in enumerate(Input):\n",
    "        Input_norm[count,:] = (i-minima['V_'+str(labels[count])])/(maxima['V_'+str(labels[count])]-minima['V_'+str(labels[count])])\n",
    "    df = pd.read_csv(path+'/normalization.csv')    \n",
    "    for i in range(n_clusters):\n",
    "        df['max V_'+str(i)] = maxima['V_'+str(i)]\n",
    "        df.to_csv(path+'/normalization.csv',index=False)\n",
    "        df['min V_'+str(i)] = minima['V_'+str(i)]\n",
    "        df.to_csv(path+'/normalization.csv',index=False)\n",
    "    return Input_norm \n",
    "\n",
    "def variance_simulations(S,t,seconds,sim):\n",
    "    x,y = np.shape(S)\n",
    "    var_mat = np.zeros((sim,y))\n",
    "    for i in range(sim):\n",
    "        var_mat[i,:] = S[(t-2)+i*seconds,:]   \n",
    "    var_arr = np.std(var_mat,axis=0)\n",
    "    #ma = np.amax(var_arr)\n",
    "    #mi = np.amin(var_arr)\n",
    "    return (var_arr)#-mi)/(ma-mi)\n",
    "    \n",
    "\n",
    "def formatNumber(num):\n",
    "    arr = []\n",
    "    for count,i in enumerate(num):\n",
    "        if i % 1 == 0:\n",
    "            arr.append(int(i))\n",
    "        else:\n",
    "            arr.append(i)\n",
    "    return arr\n",
    "\n",
    "def interpolate(x,y,z,step,method = 'cubic'):\n",
    "    xi = np.arange(0,0.0006,step)\n",
    "    yi = np.arange(0,0.0035,step)\n",
    "    xi,yi = np.meshgrid(xi,yi)\n",
    "    mask = ((xi > 0.0001) & (yi < 0.00118)) | ((xi > 7.9e-5) & (yi < 0.001784)&(yi > 0.001604))\n",
    "    # interpolate\n",
    "    zi = griddata((x,y),z,(xi,yi),method=method)\n",
    "    zi[mask] = np.nan\n",
    "    return xi,yi,zi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA ACQUISITION AND PREPARATION OF INPUT MATRICES\n",
    "You can choose the direction a or direction b. In the first case you will take blood clots data, in the second one you will take whatever data you have already prepared. The matrices S and M are collected accordingly to the paper cited at the beginning. Notice that here we take the transposed of what is said in the paper, since it is easier to deal with them like this with python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3a\n",
    "#define variables important for acquisition and preparation of data\n",
    "specie = 'vWFs'\n",
    "N_cluster = 5 #size of POD basis used for clustering\n",
    "n_clusters = 3\n",
    "n_param = 6 #including time\n",
    "mesh = 68650 #meshsize\n",
    "#choose which seconds from the simulations you want to get\n",
    "\n",
    "which_sim = np.arange(1,101,1)\n",
    "print('Simulation taken = '+str(which_sim))\n",
    "print('Number of simulation taken = '  +str(len(which_sim)))\n",
    "range_time = np.arange(2,42,1)\n",
    "rang_time = formatNumber(range_time)\n",
    "print('Time interval = '+str(range_time))\n",
    "print('Length simulation = '  +str(len(range_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4a\n",
    "#get species snaphshots from simulations, parameter matrix including time and mesh coordinates;\n",
    "#be careful to the name of the file and path, change it accordingly to where the files are!\n",
    "\n",
    "\n",
    "S = (get_field('../DoE',which_sim,rang_time,mesh,specie))\n",
    "M =(M_matrix(len(which_sim),rang_time,n_param,'../'))\n",
    "cell_centers = read_files_centers('../cellCenters')\n",
    "cell_true = []\n",
    "for i in cell_centers:\n",
    "    cell_true.append(np.array(i[1:-1].split()).astype(np.float64))\n",
    "cell_true = np.array(cell_true)\n",
    "print('Shape S = ' + str(np.shape(S)))\n",
    "print('Shape M = '+ str(np.shape(M)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5a\n",
    "#create the necessary directories\n",
    "path_cluster = '../'+specie+'/CLASSIFICATION/MODELS/'+str(N_cluster)+'/Data/'\n",
    "os.makedirs(path_cluster,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3b\n",
    "#use only if you have not used direction a!\n",
    "\n",
    "#Define S and M as above\n",
    "S = #you know it\n",
    "M = #you know it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4b #use only if you have not used direction a!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5b\n",
    "#use only if you have not used direction a!\n",
    "\n",
    "#create the necessary directories\n",
    "path_cluster = 'whatever you want'\n",
    "os.makedirs(path_cluster)\n",
    "os.makedirs(path_cluster,exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLUSTERING SECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6\n",
    "#Compute POD on S, the POD basis size is N_cluster. I suggest to keep it small\n",
    "\n",
    "_,s,v = sklearn.utils.extmath.randomized_svd(S,N_cluster,random_state = 8) #random SVD on S\n",
    "\n",
    "V_transp = v #define basis Matrix v  \n",
    "Input = np.zeros((np.shape(S)[0],N_cluster))\n",
    "print('Shape V transposed = '+ str(np.shape(V_transp)))\n",
    "print('Shape Input coefficients = '+ str(np.shape(Input)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7\n",
    "#Get the coefficients for every snapshot of S and put it in Input\n",
    "\n",
    "for count,data in enumerate(S):\n",
    "    Input[count,:] = np.matmul(V_transp,data)\n",
    "print('Shape Input coefficients = '+ str(np.shape(Input)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#8\n",
    "#Normalize every snapshot coefficients by their maximum and minimum\n",
    "\n",
    "Input_norm = np.zeros((np.shape(S)[0],N_cluster))\n",
    "for count,i in enumerate(Input):\n",
    "    Input_norm[count,:] = (i-np.amin(i))/(np.amax(i)-np.amin(i))\n",
    "Input = Input_norm.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#9\n",
    "#save input\n",
    "savetxt(path_cluster+'/coefficients.csv', Input, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10\n",
    "#perform the clustering\n",
    "\n",
    "X = Input\n",
    "fcm = FCM(n_clusters=n_clusters)\n",
    "fcm.fit(X)\n",
    "fcm_centers = fcm.centers #get the centers in a dimensional space of N dimension\n",
    "fcm_labels = fcm.predict(X) # get the labels for each set of coefficients and thus for each snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#11\n",
    "#divide in training and testing for the classification algorithm\n",
    "\n",
    "param_train = copy.copy(M[0:3800])\n",
    "labels_train = copy.copy(fcm_labels[0:3800])\n",
    "param_test = copy.copy(M[-200:])\n",
    "labels_test = copy.copy(fcm_labels[-200:])\n",
    "print('Shape param_train = ' + str(np.shape(param_train)))\n",
    "print('Shape param_test = ' + str(np.shape(param_test)))\n",
    "print('Shape labels_train = ' + str(np.shape(labels_train)))\n",
    "print('Shape labels_test = ' + str(np.shape(labels_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#12\n",
    "#normalize parameters and save in a file maxima and minima of every feature \n",
    "\n",
    "param_train = normalize_M(param_train,path_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#13\n",
    "#save training matrices\n",
    "\n",
    "savetxt(path_cluster+'/param.csv', param_train, delimiter=',')\n",
    "savetxt(path_cluster+'/labels.csv', labels_train, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#14\n",
    "#save test matrices\n",
    "\n",
    "savetxt(path_cluster+'/param_test.csv', param_test, delimiter=',')\n",
    "savetxt(path_cluster+'/label_test.csv', labels_test, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "END OF CLUSTERING SECTION. NOW YOU SHOULD RUN THE CLASSIFIER IN ../code_CLASSIFICATION/main_training.py. After that you have a classifier. Then you can proceed to the next section to prepare data for POD-DL-ROM based on clustering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATE THE N_CLUSTERS POD BASIS BASED ON FCM_LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#15a\n",
    "#create path for POD-DL-ROM if you use blood clots\n",
    "N = 64\n",
    "path_pod_dl_rom = '../'+specie+'/MODELS/'+str(N)+'/Data/'\n",
    "os.makedirs(path_pod_dl_rom,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#15b\n",
    "#create path for POD-DL-ROM if you use blood clots\n",
    "\n",
    "path_pod_dl_rom = 'whatever you want'\n",
    "os.makedirs(path_pod_dl_rom,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#16\n",
    "# divide snapshots (not coefficients!) and parameters vectors in train and test\n",
    "\n",
    "S_train = copy.copy(S[0:3800])\n",
    "S_test = copy.copy(S[-200:])\n",
    "M_train = copy.copy(M[0:3800])\n",
    "M_test = copy.copy(M[-200:])\n",
    "print('Shape S_train = ' + str(np.shape(S_train)))\n",
    "print('Shape S_test = ' + str(np.shape(S_test)))\n",
    "print('Shape M_train = ' + str(np.shape(M_train)))\n",
    "print('Shape M_test = ' + str(np.shape(M_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#17\n",
    "#create the n_cluster snapshot matrices. Notice the labels_train from the clustering!\n",
    "\n",
    "clusters = {}\n",
    "for i in range(n_clusters):\n",
    "    clusters['V_'+str(i)] = []\n",
    "for count, data in enumerate(S_train):\n",
    "    clusters['V_'+str(labels_train[count])].append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#18\n",
    "#create the n_cluster POD basis with their singular values s!\n",
    "\n",
    "pod_basis = {}\n",
    "for i in range(n_clusters):\n",
    "    _,s,v = sklearn.utils.extmath.randomized_svd(np.array(clusters['V_'+str(i)]),N,random_state = 8)\n",
    "    pod_basis['V_'+str(i)] = v,s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#19\n",
    "#create coefficient matrix Input. The coefficient will be obtained by projecting onto the correct basis!\n",
    "#at the same time we get the max and the min for every cluster, they will be used for the normalization\n",
    "\n",
    "maxima = {}\n",
    "minima = {}\n",
    "\n",
    "for i in range(n_clusters):\n",
    "    maxima['V_'+str(i)] = 0\n",
    "    minima['V_'+str(i)] = 0\n",
    "\n",
    "Input = np.zeros((np.shape(S_train)[0],N))\n",
    "for count,data in enumerate(S_train):\n",
    "    projection =  np.matmul(pod_basis['V_'+str(labels_train[count])][0],data)\n",
    "    Input[count,:] = projection\n",
    "    ma = np.amax(projection)\n",
    "    mi = np.amin(projection)\n",
    "    if np.amax(projection)>maxima['V_'+str(labels_train[count])]:\n",
    "        maxima['V_'+str(labels_train[count])] = ma\n",
    "    if np.amin(projection)<minima['V_'+str(labels_train[count])]:\n",
    "        minima['V_'+str(labels_train[count])] = mi\n",
    "\n",
    "print('Input matrix shape = '+str(np.shape(Input)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#19\n",
    "#normalize data and save on file normalization the max and the min\n",
    "#both of parameter matrix and of POD coefficients for every basis\n",
    "\n",
    "M_train = normalize_M(M_train,path_pod_dl_rom)\n",
    "Input = normalize_Input(Input,path_pod_dl_rom,maxima,minima,n_clusters,labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#20\n",
    "#save training matrices\n",
    "\n",
    "savetxt(path_pod_dl_rom+'/Input.csv', Input, delimiter=',')\n",
    "savetxt(path_pod_dl_rom+'/M_train.csv',M_train,delimiter = ',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#21\n",
    "#save test matrices\n",
    "savetxt(path_pod_dl_rom+'/S_test.csv', S_test, delimiter=',')\n",
    "savetxt(path_pod_dl_rom+'/M_test.csv', M_test, delimiter=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#22\n",
    "#save labels test\n",
    "savetxt(path_pod_dl_rom+'/label_test.csv', labels_test, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#23\n",
    "#save POD MATRICES\n",
    "\n",
    "for i in range(n_clusters):\n",
    "    savetxt(path_pod_dl_rom+'/V_'+str(i)+'.csv', pod_basis['V_'+str(i)][0].transpose(), delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should send the directory path_pod_dl_rom to the cluster and run main_training.py of the directory LOCALIZED-POD-DL-ROM/code_LOC-POD-DL-ROM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END OF DATA ACQUISITION, NOW YOU SHOULD CHECK THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize some modes\n",
    "fig,ax = plt.subplots(1,3,figsize = (20,10))\n",
    "cm = plt.cm.get_cmap('RdYlBu_r')\n",
    "#ax = fig.gca(projection='3d')\n",
    "scatter_plot = ax[0].scatter(cell_true[:,0], cell_true[:,1], c = pod_basis['V_0'][0][0,:], lw=0, s=20,cmap=cm)\n",
    "plt.colorbar(scatter_plot, ax=ax[0])\n",
    "scatter_plot = ax[1].scatter(cell_true[:,0], cell_true[:,1], c = pod_basis['V_2'][0][0,:], lw=0, s=20,cmap=cm)\n",
    "plt.colorbar(scatter_plot, ax=ax[1])\n",
    "scatter_plot = ax[2].scatter(cell_true[:,0], cell_true[:,1], c = pod_basis['V_1'][0][0,:], lw=0, s=20,cmap=cm)\n",
    "plt.colorbar(scatter_plot, ax=ax[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = np.zeros((40,256))\n",
    "for i in range(40):\n",
    "    v = variance_simulations(X,i+2,40,100)\n",
    "    V[i,:] = v\n",
    "savetxt(path+'/CLASSIFICATION'+'/MODELS/'+dir_cst+'/Data/'+'/std.csv', V, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fcm_labels[0:40],'o')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(M)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
