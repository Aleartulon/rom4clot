{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "086cc079",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import scipy.interpolate\n",
    "import os\n",
    "import csv\n",
    "from scipy.interpolate import griddata\n",
    "from numpy import savetxt\n",
    "import sklearn.utils.extmath\n",
    "import progressbar\n",
    "from fcmeans import FCM\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c700bcb9",
   "metadata": {},
   "source": [
    "# FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89089ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_field(path,simulations,seconds,mesh,field):\n",
    "    Ns = len(simulations)*len(seconds)\n",
    "    S = np.zeros((Ns,mesh))\n",
    "    row = 0\n",
    "    bar = progressbar.ProgressBar(maxval=len(simulations), \\\n",
    "    widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n",
    "    bar.start()\n",
    "    for i,s in enumerate(simulations):\n",
    "        bar.update(i+1)\n",
    "        for t in seconds:\n",
    "            data = get_data(path+'/'+'sim_'+str(s)+'/'+str(t)+'/'+field,mesh)\n",
    "            S[row,:] = data\n",
    "            row+=1\n",
    "        sleep(0.1)\n",
    "    bar.finish()\n",
    "    return S\n",
    "\n",
    "def get_data(path,mesh):\n",
    "    with open(path, 'r') as file:\n",
    "        output = [None]*mesh\n",
    "        count = 0\n",
    "        start = False\n",
    "        for line in file:\n",
    "            #print(str(line))\n",
    "            if line[0] == ')':\n",
    "                break\n",
    "            if start:\n",
    "                output[count] = (float(line))\n",
    "                #print(output[count])\n",
    "                count +=1\n",
    "            if line[0] == \"(\":\n",
    "                start = True\n",
    "        return np.array(output)\n",
    "\n",
    "def read_files_centers(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        if file_path[-1] != 'U':\n",
    "            lines = np.array(file.read().split(\"\\n\"))\n",
    "            start = np.where(lines == \"(\")\n",
    "            end = np.where(lines == \")\")\n",
    "            if len(start[0]) != 0:\n",
    "                output = np.array(lines[start[0][0]+1:end[0][0]])\n",
    "                return output\n",
    "            else:\n",
    "                return []\n",
    "        else:\n",
    "            U = read_U(file_path)\n",
    "            return U\n",
    "\n",
    "def M_matrix(simulations,time,parameters,path):\n",
    "    file = open(path+'/X_LHS_Uniform.csv')\n",
    "    csvreader = csv.reader(file)\n",
    "    #header = next(csvreader)\n",
    "    #print(header)\n",
    "    rows = []\n",
    "    for count,row in enumerate(csvreader):\n",
    "        if count+1<=simulations:\n",
    "            rows.append(row)\n",
    "        else: \n",
    "            break\n",
    "    file.close()\n",
    "    M = np.zeros((simulations*len(time),parameters))#parameters = parameters+1\n",
    "    count = 0\n",
    "    for i in rows:\n",
    "        for t in time:\n",
    "            first = np.array([t])\n",
    "            second = np.array(i)\n",
    "            #print(count*time+t)\n",
    "            M[count,:] = np.concatenate([first,second])\n",
    "            count+=1\n",
    "    return M\n",
    "\n",
    "def normalize_M(M):\n",
    "    M_norm = np.zeros((np.shape(M)[0],np.shape(M)[1]))\n",
    "    M_max = np.amax(M, axis=0)\n",
    "    M_min = -np.amax(-M,axis=0)\n",
    "    for i in range(np.shape(M)[0]):\n",
    "        for j in range(np.shape(M)[1]):\n",
    "            M_norm[i,j] = (M[i,j]- M_min[j])/(M_max[j]-M_min[j])\n",
    "    return M_norm\n",
    "\n",
    "def normalize_S(S):\n",
    "    S_max = np.amax(S)\n",
    "    S_min = -np.amax(-S)\n",
    "    print(S_max)\n",
    "    print(S_min)\n",
    "    return (S-S_min)/(S_max-S_min)\n",
    "        \n",
    "\n",
    "def variance_simulations(S,t,seconds,sim):\n",
    "    x,y = np.shape(S)\n",
    "    var_mat = np.zeros((sim,y))\n",
    "    for i in range(sim):\n",
    "        var_mat[i,:] = S[(t-2)+i*seconds,:]   \n",
    "    var_arr = np.std(var_mat,axis=0)\n",
    "    ma = np.amax(var_arr)\n",
    "    mi = np.amin(var_arr)\n",
    "    return (var_arr-mi)/(ma-mi)\n",
    "    \n",
    "\n",
    "def formatNumber(num):\n",
    "    arr = []\n",
    "    for count,i in enumerate(num):\n",
    "        if i % 1 == 0:\n",
    "            arr.append(int(i))\n",
    "        else:\n",
    "            arr.append(i)\n",
    "    return arr\n",
    "\n",
    "def interpolate(x,y,z,step,method = 'cubic'):\n",
    "    xi = np.arange(0,0.0006,step)\n",
    "    yi = np.arange(0,0.0035,step)\n",
    "    xi,yi = np.meshgrid(xi,yi)\n",
    "    mask = ((xi > 0.0001) & (yi < 0.00118)) | ((xi > 7.9e-5) & (yi < 0.001784)&(yi > 0.001604))\n",
    "    # interpolate\n",
    "    zi = griddata((x,y),z,(xi,yi),method=method)\n",
    "    zi[mask] = np.nan\n",
    "    return xi,yi,zi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b9f8d5",
   "metadata": {},
   "source": [
    "# DATA ACQUISITION AND PREPARATION OF INPUT MATRICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78d49b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time interval = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]\n",
      "Length simulation = 40\n"
     ]
    }
   ],
   "source": [
    "#define variables important for acquisition and preparation of data\n",
    "specie = 'vWFs'\n",
    "N = 256 #size of POD basis\n",
    "n_clusters = 3\n",
    "n_param = 6\n",
    "#choose which seconds from the simulations you want to get\n",
    "rang = np.arange(2,42,1)\n",
    "rang = formatNumber(rang)\n",
    "print('Time interval = '+str(rang))\n",
    "print('Length simulation = '  +str(len(rang)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb6afad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[=========================================================================] 100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape S = (4000, 68650)\n",
      "Shape M = (4000, 6)\n"
     ]
    }
   ],
   "source": [
    "#get species snaphshots from simulations, parameter matrix including time and mesh coordinates;\n",
    "#be careful to the name of the file and path, change it accordingly yo where the files are!\n",
    "\n",
    "dir = '3_cluster_256_N'\n",
    "path_data = '../POD-clot/'\n",
    "S = (get_field(path_data+'DoE2',np.arange(1,101,1),rang,68650,specie))\n",
    "M =(M_matrix(100,rang,n_param,path_data))\n",
    "cell_centers = read_files_centers(path_data+'cellCenters')\n",
    "cell_true = []\n",
    "for i in cell_centers:\n",
    "    cell_true.append(np.array(i[1:-1].split()).astype(np.float64))\n",
    "cell_true = np.array(cell_true)\n",
    "print('Shape S = ' + str(np.shape(S)))\n",
    "print('Shape M = '+ str(np.shape(M)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52608ecd",
   "metadata": {},
   "source": [
    "# CLUSTERING SECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "929843d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape V transposed = (256, 68650)\n",
      "Shape Input coefficients = (4000, 256)\n"
     ]
    }
   ],
   "source": [
    "_,s,v = sklearn.utils.extmath.randomized_svd(S,N,random_state = 8) #random SVD on S\n",
    "\n",
    "V_transp = v#define basis Matrix v  \n",
    "Input = np.zeros((np.shape(S)[0],N))\n",
    "print('Shape V transposed = '+ str(np.shape(V_transp)))\n",
    "print('Shape Input coefficients = '+ str(np.shape(Input)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93fe1815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape Input coefficients = (4000, 256)\n"
     ]
    }
   ],
   "source": [
    "#Get the coefficients for every snapshot of S and put it in Input\n",
    "for count,data in enumerate(S):\n",
    "    Input[count,:] = np.matmul(V_transp,data)\n",
    "print('Shape Input coefficients = '+ str(np.shape(Input)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ee63954",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize every snapshot coeffifients by their maximum and minimum\n",
    "Input_norm = np.zeros((np.shape(S)[0],N))\n",
    "for count,i in enumerate(Input):\n",
    "    Input_norm[count,:] = (i-np.amin(i))/(np.amax(i)-np.amin(i))\n",
    "Input = Input_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53aa6929",
   "metadata": {},
   "outputs": [],
   "source": [
    "#perfoem the clustering\n",
    "X = Input\n",
    "fcm = FCM(n_clusters=n_clusters)\n",
    "fcm.fit(X)\n",
    "fcm_centers = fcm.centers #get the centers in a dimensional space of N dimension\n",
    "fcm_labels = fcm.predict(X) # get the labels for each set of coefficients and thus for each snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333ea8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#divide in training and testing for the classification algorithm\n",
    "param_train = M[0:3800].copy()\n",
    "labels_train = fcm_labels[0:3800].copy()\n",
    "params_test = M[:200].copy()\n",
    "labels_test = fcm_labels[:200].copy()\n",
    "print('Shape param_train = ' + str(np.shape(param_train)))\n",
    "print('Shape param_test = ' + str(np.shape(param_train)))\n",
    "print('Shape label_train = ' + str(np.shape(param_train)))\n",
    "print('Shape label_test = ' + str(np.shape(param_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b2810a",
   "metadata": {},
   "source": [
    "# CREATE THE N_CLUSTERS POD BASIS BASED ON FCM_LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b5efd0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape S_train = (3800, 68650)\n",
      "Shape S_test = (200, 68650)\n",
      "Shape M_train = (3800, 6)\n",
      "Shape M_test = (200, 6)\n"
     ]
    }
   ],
   "source": [
    "# divide snapshots(not coefficietnts!) and parameters vectors in train and test\n",
    "\n",
    "S_train = S[0:3800]\n",
    "S_test = S[-200:]\n",
    "M_train = M[0:3800]\n",
    "M_test = M[-200:]\n",
    "print('Shape S_train = ' + str(np.shape(S_train)))\n",
    "print('Shape S_test = ' + str(np.shape(S_test)))\n",
    "print('Shape M_train = ' + str(np.shape(M_train)))\n",
    "print('Shape M_test = ' + str(np.shape(M_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f8af2bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4]\n",
      "[4 1]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([0,1,2,3,4])\n",
    "b = a[0:2].copy()\n",
    "b[0] = 4\n",
    "print(a)\n",
    "print(b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
